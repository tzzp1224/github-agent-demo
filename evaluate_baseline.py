# æ–‡ä»¶è·¯å¾„: evaluate_baseline.py
import asyncio
import json
import sys
import os

# ç¡®ä¿èƒ½å¯¼å…¥ app æ¨¡å—
sys.path.append(os.getcwd())

from app.services.vector_service import vector_db
from app.services.github_service import get_file_content
from app.core.config import settings
from app.services.chunking_service import PythonASTChunker

# ç›®æ ‡ä»“åº“
REPO_URL = "https://github.com/fastapi/fastapi"

async def run_evaluation():
    print("ğŸ§ª --- å¼€å§‹ RAG åŸºçº¿è¯„ä¼° ---")
    
    # 1. åŠ è½½æ•°æ®é›†
    try:
        with open("evaluation/golden_dataset.json", "r", encoding="utf-8") as f:
            dataset = json.load(f)
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°æ•°æ®é›†ï¼Œè¯·å…ˆåˆ›å»º evaluation/golden_dataset.json")
        return

    # 2. å‡†å¤‡ç¯å¢ƒ (é‡ç½®å‘é‡åº“)
    vector_db.reset_collection()
    
    # 3. æ„å»ºç´¢å¼• (Indexing)
    # ä¸ºäº†æµ‹è¯• Retrieve èƒ½åŠ›ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ç­”æ¡ˆæ–‡ä»¶åœ¨åº“é‡Œã€‚
    # è¿™é‡Œæˆ‘ä»¬æ”¶é›†æ•°æ®é›†ä¸­æåˆ°çš„æ‰€æœ‰æ–‡ä»¶ï¼Œæ¨¡æ‹Ÿå®ƒä»¬å·²ç»è¢« Agent é€‰ä¸­å¹¶ç´¢å¼•äº†ã€‚
    target_files = list(set([item["answer_file"] for item in dataset]))
    
    print(f"ğŸ“¦ [AST Mode] æ­£åœ¨å‡†å¤‡æµ‹è¯•æ•°æ®...")
    
    documents = []
    metadatas = []
    chunker = PythonASTChunker(min_chunk_size=50) # åˆå§‹åŒ–åˆ‡åˆ†å™¨
    
    for file_path in target_files:
        print(f"   â¬‡ï¸ ä¸‹è½½å¹¶ASTåˆ‡åˆ†: {file_path}")
        content = get_file_content(REPO_URL, file_path)
        if content:
            # === æ ¸å¿ƒä¿®æ”¹ç‚¹ ===
            # æ—§é€»è¾‘: snippet = content[:1000]
            # æ–°é€»è¾‘: ä½¿ç”¨ AST åˆ‡åˆ†å‡ºå¤šä¸ªå®Œæ•´çš„å—
            file_chunks = chunker.chunk_file(content, file_path)
            
            for chunk in file_chunks:
                documents.append(chunk["content"])
                # åˆå¹¶å…ƒæ•°æ®ï¼Œä¿ç•™æ–‡ä»¶å
                meta = chunk["metadata"]
                # ChromaDB çš„ metadata å€¼å¿…é¡»æ˜¯ str, int, float, bool
                # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç¡®ä¿ file å­—æ®µå­˜åœ¨
                # æ–°ä»£ç : å¢åŠ  class å­—æ®µï¼Œå¹¶ç¡®ä¿è½¬ä¸ºå­—ç¬¦ä¸² (ChromaDB è¦æ±‚ metadata å€¼ä¸ºç®€å•ç±»å‹)
                metadatas.append({
                    "file": meta["file"], 
                    "type": meta["type"], 
                    "name": meta.get("name", ""),
                    "class": meta.get("class") or "" # å¤„ç† None
                })
                
        else:
            print(f"   âš ï¸ è­¦å‘Š: æ— æ³•ä¸‹è½½ {file_path}")

    # å†™å…¥å‘é‡åº“
    vector_db.add_documents(documents, metadatas)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œå¼€å§‹æµ‹è¯•æ£€ç´¢...")
    print("-" * 30)

    # 4. æ‰§è¡Œè¯„ä¼° (Evaluation)
    hits = 0
    total = len(dataset)
    top_k = 3

    for item in dataset:
        query = item["query"]
        expected_file = item["answer_file"]
        
        # è°ƒç”¨ç°æœ‰çš„æœç´¢æ¥å£
        results = vector_db.search(query, top_k=top_k)
        
        # æ£€æŸ¥å‘½ä¸­æƒ…å†µ
        retrieved_files = [res['file'] for res in results]
        is_hit = expected_file in retrieved_files
        
        if is_hit:
            hits += 1
            status = "âœ… å‘½ä¸­"
        else:
            status = "âŒ æœªå‘½ä¸­"
            
        print(f"Q: {query[:40]}...")
        print(f"   æœŸæœ›: {expected_file}")
        print(f"   æ£€ç´¢: {retrieved_files}")
        print(f"   ç»“æœ: {status}\n")

    # 5. è¾“å‡ºæŠ¥å‘Š
    hit_rate = (hits / total) * 100
    print("=" * 30)
    print(f"ğŸ“Š åŸºçº¿è¯„ä¼°ç»“æœ (Baseline)")
    print(f"ğŸ¯ Hit Rate @ {top_k}: {hit_rate:.2f}%")
    print("=" * 30)
    
    # å»ºè®®ï¼šå°†ç»“æœå†™å…¥æ–‡ä»¶ä»¥ä¾¿åç»­å¯¹æ¯”
    with open("evaluation/baseline_result.txt", "w") as f:
        f.write(f"Baseline Hit Rate: {hit_rate:.2f}%")

if __name__ == "__main__":
    # æ£€æŸ¥ Key
    if not settings.GEMINI_API_KEY:
        print("âŒ è¯·å…ˆé…ç½® .env ä¸­çš„ GEMINI_API_KEY")
    else:
        asyncio.run(run_evaluation())