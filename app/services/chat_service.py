# æ–‡ä»¶è·¯å¾„: app/services/chat_service.py
import json
import asyncio
import re
from app.core.config import settings
from app.utils.llm_client import client
from app.services.vector_service import store_manager
from app.services.github_service import get_file_content
from app.services.chunking_service import PythonASTChunker

chunker = PythonASTChunker(min_chunk_size=100)

async def process_chat_stream(user_query: str, session_id: str):
    """
    æµå¼å¤„ç†èŠå¤©è¯·æ±‚ï¼Œæ”¯æŒåŠ¨æ€åŠ è½½å’Œå®æ—¶åé¦ˆ
    """
    vector_db = store_manager.get_store(session_id)
    
    # 1. åˆæ¬¡æ£€ç´¢
    relevant_docs = vector_db.search_hybrid(user_query, top_k=5)

    # # === ğŸ”DEBUG ä»£ç å¼€å§‹ ===
    # print("\n" + "="*50)
    # print(f"ğŸ§ [DEBUG] ç”¨æˆ·æé—®: {user_query}")
    # print(f"ğŸ“Š [DEBUG] æ£€ç´¢å‘½ä¸­ {len(relevant_docs)} ä¸ªç‰‡æ®µ:")
    # for i, doc in enumerate(relevant_docs):
    #     # ä½¿ç”¨ .get() é˜²æ­¢ KeyErrorï¼Œè™½ç„¶ä¸Šé¢ä¿®å¥½äº†ï¼Œä½†è¿™æ ·æ›´å®‰å…¨
    #     meta = doc.get('metadata', {}) 
        
    #     print(f"  Result {i+1}:")
    #     print(f"    - File: {meta.get('file', 'Unknown')}")
    #     print(f"    - Type: {meta.get('type', 'unknown')}") 
    #     print(f"    - ClassCtx: {meta.get('class', 'None')}")
    #     # æ‰“å°å‰ 50 ä¸ªå­—ç¬¦é¢„è§ˆ
    #     content_preview = doc.get('content', '')[:50].replace('\n', ' ')
    #     print(f"    - Content Preview: {content_preview}...") 
    # print("="*50 + "\n")
    # # === ğŸ”DEBUG ä»£ç ç»“æŸ ===
    
    context_str = _build_context(relevant_docs)
    
    # 2. æ„é€  Prompt
    system_instruction = """
    You are a Code Expert. 
    
    [Rules]
    1. Answer based on Context.
    2. If the code exists in Context -> Just answer directly.
    3. If the specific file is MISSING in Context but you know the path -> Output ONLY JSON: {"missing_file": "path/to/file.py"}
    
    [Critical Strategy for "Summary" Questions]
    If the user asks "What is in file X?" or "Summarize file X", and you only see a few functions from X in the Context:
    -> This means you are seeing incomplete fragments.
    -> You MUST request to read the file again to get the FULL content.
    -> Output JSON: {"missing_file": "path/to/file.py"}
    """
    
    prompt = f"""
    {system_instruction}
    
    Context:
    {context_str}
    
    User Query: {user_query}
    """
    
    if not client: 
        yield "âŒ LLM Error: Client not initialized"
        return

    try:
        # === æ ¸å¿ƒä¿®æ”¹ï¼šç¬¬ä¸€æ¬¡è°ƒç”¨æ”¹ä¸ºæµå¼ (generate_content_stream) ===
        stream = client.models.generate_content_stream(
            model=settings.MODEL_NAME,
            contents=prompt
        )
        
        # === æ™ºèƒ½ç¼“å†²é€»è¾‘ ===
        buffer = ""
        is_checking_json = True # æ ‡è®°æ˜¯å¦è¿˜åœ¨æ£€æµ‹ JSON é˜¶æ®µ
        is_tool_call = False    # æ ‡è®°æœ€ç»ˆæ˜¯å¦ç¡®è®¤ä¸ºå·¥å…·è°ƒç”¨
        
        for chunk in stream:
            text_chunk = chunk.text
            
            if is_checking_json:
                buffer += text_chunk
                # æ¸…æ´— buffer ä»¥å‰ç¼€æ£€æŸ¥
                clean_start = buffer.strip().replace("```json", "").replace("```", "").strip()
                
                # å¦‚æœç¼“å†²åŒºè¿˜å¾ˆçŸ­ï¼Œç»§ç»­ç§¯æ”’ (é˜²æ­¢è¯¯åˆ¤)
                if len(clean_start) < 5:
                    continue
                    
                # æ£€æŸ¥ç‰¹å¾
                if clean_start.startswith("{"):
                    # çœ‹èµ·æ¥åƒ JSONï¼Œç»§ç»­ç¼“å†²ï¼Œä¸è¾“å‡ºç»™ç”¨æˆ·
                    continue 
                else:
                    # ç¡®å®šä¸æ˜¯ JSONï¼Œæ˜¯æ™®é€šå›ç­”ï¼
                    # 1. æŠŠç§¯æ”’çš„ buffer åå‡ºå»
                    yield buffer
                    buffer = "" # æ¸…ç©º
                    is_checking_json = False # åœæ­¢æ£€æµ‹ï¼Œåç»­ç›´æ¥é€ä¼ 
            else:
                # å·²ç»ç¡®å®šæ˜¯æ™®é€šæ–‡æœ¬ï¼Œç›´æ¥æµå¼è¾“å‡º
                yield text_chunk

        # æµç»“æŸäº†
        # å¦‚æœ is_checking_json ä¾ç„¶ä¸º Trueï¼Œè¯´æ˜ LLM å›å¤å¾ˆçŸ­æˆ–è€…å…¨æ˜¯ JSON
        missing_file = None
        if is_checking_json and buffer:
            # å°è¯•è§£æ JSON
            clean_text = buffer.strip().replace("```json", "").replace("```", "").strip()
            if "missing_file" in clean_text:
                match = re.search(r"\{.*?\}", clean_text, re.DOTALL)
                if match:
                    try:
                        data = json.loads(match.group(0))
                        missing_file = data.get("missing_file")
                        is_tool_call = True
                    except:
                        pass
            
            # å¦‚æœä¸æ˜¯ JSONï¼Œè¯´æ˜æ˜¯ä¸€å¥å¾ˆçŸ­çš„è¯ï¼ŒæŠŠå®ƒè¡¥å‘ç»™ç”¨æˆ·
            if not is_tool_call:
                yield buffer

        # === åˆ†æ”¯ A: è§¦å‘åŠ¨æ€åŠ è½½ (ReAct) ===
        if is_tool_call and missing_file:
            # å®æ—¶åé¦ˆç»™å‰ç«¯
            yield f"> ğŸ¤” å‘ç°ç¼ºå°‘æ–‡ä»¶: `{missing_file}`\n\n"
            
            if not vector_db.repo_url:
                yield f"> âš ï¸ ä¼šè¯ä¿¡æ¯ä¸¢å¤± (Repo URL)ï¼Œæ— æ³•ä¸‹è½½ã€‚\n\n"
                return

            new_docs_content = []
            
            # æ£€æŸ¥å·²ç´¢å¼•
            if missing_file in vector_db.indexed_files:
                yield f"> ğŸ“š è¯¥æ–‡ä»¶å·²åœ¨çŸ¥è¯†åº“ä¸­ï¼Œæ­£åœ¨æå–ç»†èŠ‚...\n\n"
                stored_docs = vector_db.get_documents_by_file(missing_file)
                if stored_docs:
                    new_docs_content = stored_docs
                else:
                    yield f"> âš ï¸ ç´¢å¼•ä¸­æœªæ‰¾åˆ°å†…å®¹ï¼Œå°è¯•é‡æ–°ä¸‹è½½...\n\n"
            
            # ä¸‹è½½
            if not new_docs_content:
                yield f"> ğŸ“¥ æ­£åœ¨ä¸‹è½½å¹¶åˆ†æ: `{missing_file}`...\n\n"
                success = await _download_and_index(vector_db, missing_file)
                if success:
                    new_docs_content = vector_db.get_documents_by_file(missing_file)
                else:
                    yield f"> âŒ ä¸‹è½½å¤±è´¥ (æ–‡ä»¶ä¸å­˜åœ¨æˆ–ç½‘ç»œé”™è¯¯)ã€‚\n\n"
                    # è¿™é‡Œå¯ä»¥é€‰æ‹©æŠŠåŸå§‹ buffer (JSON) æ‰“å°å‡ºæ¥ï¼Œæˆ–è€…å¿½ç•¥
                    return

            # === äºŒæ¬¡ç”Ÿæˆ (Streaming) ===
            supplementary_context = _build_context(new_docs_content)
            
            retry_prompt = f"""
            System: You requested '{missing_file}'. Here is its content.
            Now answer the user's question based on the updated context.
            
            New File Content:
            {supplementary_context}
            
            Original Context:
            {context_str}
            
            User Query: {user_query}
            """
            
            # ç¬¬äºŒæ¬¡æµå¼è°ƒç”¨
            stream_retry = client.models.generate_content_stream(
                model=settings.MODEL_NAME,
                contents=retry_prompt
            )
            for chunk in stream_retry:
                yield chunk.text
                await asyncio.sleep(0.01)

    except Exception as e:
        import traceback
        traceback.print_exc()
        yield f"âŒ Error: {str(e)}"

# è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜)
def _build_context(docs):
    if not docs: return "No code found."
    context = ""
    for doc in docs:
        file_info = doc['file']
        if 'class' in doc.get('metadata', {}):
            cls = doc['metadata']['class']
            if cls: file_info += f" (Class: {cls})"
        context += f"\n--- File: {file_info} ---\n{doc['content'][:2000]}\n"
    return context

async def _download_and_index(vector_db, file_path):
    try:
        content = get_file_content(vector_db.repo_url, file_path)
        if not content: return False
        
        chunks = await asyncio.to_thread(chunker.chunk_file, content, file_path)
        if not chunks: return False
        
        documents = [c["content"] for c in chunks]
        metadatas = []
        for c in chunks:
            meta = c["metadata"]
            metadatas.append({
                "file": meta["file"],
                "type": meta["type"],
                "name": meta.get("name", ""),
                "class": meta.get("class") or ""
            })
            
        await asyncio.to_thread(vector_db.add_documents, documents, metadatas)
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False